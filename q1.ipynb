{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ViT CIFAR-10 (PyTorch) - Colab-ready\n",
        "# Requirements: torch, torchvision (Colab preinstalls these). Optionally tqdm.\n",
        "# Run in Colab with GPU runtime selected.\n",
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------\n",
        "# Config / Hyperparameters\n",
        "# -------------------------\n",
        "class Config:\n",
        "    seed = 42\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    data_dir = \"./data\"\n",
        "    num_workers = 4\n",
        "    batch_size = 256                # adjust if GPU memory small\n",
        "    epochs = 120\n",
        "    image_size = 32                 # CIFAR-10 images are 32x32\n",
        "    patch_size = 4                  # for CIFAR-10: 4 -> 8x8 patches (64 tokens). 16 is too big for 32x32.\n",
        "    in_channels = 3\n",
        "    embed_dim = 256                 # model width\n",
        "    depth = 12                      # number of transformer encoder blocks\n",
        "    num_heads = 8\n",
        "    mlp_ratio = 4.0\n",
        "    dropout = 0.1\n",
        "    attn_dropout = 0.0\n",
        "    weight_decay = 0.05\n",
        "    lr = 3e-4\n",
        "    betas = (0.9, 0.999)\n",
        "    warmup_epochs = 10\n",
        "    use_amp = True                  # mixed precision\n",
        "    save_dir = \"./checkpoints\"\n",
        "    print_freq = 100\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# -------------------------\n",
        "# Reproducibility\n",
        "# -------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(cfg.seed)\n",
        "os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Patch Embedding\n",
        "# -------------------------\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=256):\n",
        "        super().__init__()\n",
        "        assert img_size % patch_size == 0, \"Image size must be divisible by patch size.\"\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = img_size // patch_size\n",
        "        self.num_patches = self.grid_size * self.grid_size\n",
        "        # Implement patchify with Conv2d: kernel & stride = patch_size\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        # conv output shape: (B, embed_dim, grid, grid) -> flatten to (B, num_patches, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, H, W)\n",
        "        x = self.proj(x)  # (B, embed_dim, G, G)\n",
        "        B, E, G, G2 = x.shape\n",
        "        assert G == G2\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
        "        return x\n",
        "\n",
        "# -------------------------\n",
        "# Transformer Blocks\n",
        "# -------------------------\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, dropout=0.0):\n",
        "        super().__init__()\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.0, attn_dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=attn_dropout, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = MLP(embed_dim, hidden_features=hidden_dim, dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, N, E)\n",
        "        y = self.norm1(x)\n",
        "        y, _ = self.attn(y, y, y, need_weights=False)\n",
        "        x = x + self.dropout1(y)\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "# -------------------------\n",
        "# Vision Transformer\n",
        "# -------------------------\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=32,\n",
        "                 patch_size=4,\n",
        "                 in_chans=3,\n",
        "                 num_classes=10,\n",
        "                 embed_dim=256,\n",
        "                 depth=12,\n",
        "                 num_heads=8,\n",
        "                 mlp_ratio=4.0,\n",
        "                 dropout=0.1,\n",
        "                 attn_dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size,\n",
        "                                      in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # CLS token\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        # Learnable pos embedding for (num_patches + 1)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "\n",
        "        self.pos_drop = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout, attn_dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "\n",
        "        # Classifier head\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Initialization\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        # initialize head\n",
        "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
        "        if self.head.bias is not None:\n",
        "            nn.init.zeros_(self.head.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, H, W)\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)  # (B, N, E)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, E)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # (B, N+1, E)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        cls_out = x[:, 0]  # (B, E)\n",
        "        logits = self.head(cls_out)\n",
        "        return logits\n",
        "\n",
        "# -------------------------\n",
        "# Data: CIFAR-10 + Transforms\n",
        "# -------------------------\n",
        "def get_dataloaders(batch_size, num_workers=4):\n",
        "    # Data augmentation tuned for CIFAR-10 training a ViT from scratch\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
        "                             std=(0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
        "                             std=(0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    train_dataset = datasets.CIFAR10(cfg.data_dir, train=True, download=True, transform=train_transform)\n",
        "    test_dataset = datasets.CIFAR10(cfg.data_dir, train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append((correct_k.mul_(100.0 / batch_size)).item())\n",
        "        return res\n",
        "\n",
        "# -------------------------\n",
        "# LR schedule: Cosine + linear warmup\n",
        "# -------------------------\n",
        "def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0.0):\n",
        "    warmup_schedule = []\n",
        "    if warmup_epochs > 0:\n",
        "        warmup_iters = warmup_epochs * niter_per_ep\n",
        "        warmup_schedule = list(torch.linspace(start_warmup_value, base_value, warmup_iters))\n",
        "    iters = epochs * niter_per_ep - len(warmup_schedule)\n",
        "    if iters > 0:\n",
        "        schedule = final_value + 0.5 * (base_value - final_value) * (1 + torch.cos(torch.linspace(0, math.pi, iters)))\n",
        "        schedule = schedule.tolist()\n",
        "    else:\n",
        "        schedule = []\n",
        "    return warmup_schedule + schedule\n",
        "\n",
        "# -------------------------\n",
        "# Train / Eval loops\n",
        "# -------------------------\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, scaler=None, scheduler=None):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    pbar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Train Epoch {epoch}\")\n",
        "    for i, (images, targets) in pbar:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(images)\n",
        "                loss = F.cross_entropy(outputs, targets)\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler()\n",
        "\n",
        "        acc1 = accuracy(outputs, targets, topk=(1,))[0]\n",
        "        running_loss = (running_loss * i + loss.item()) / (i + 1)\n",
        "        running_acc = (running_acc * i + acc1) / (i + 1)\n",
        "        if (i + 1) % cfg.print_freq == 0 or i == len(data_loader) - 1:\n",
        "            pbar.set_postfix(loss=running_loss, acc=running_acc)\n",
        "    return running_loss, running_acc\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (images, targets) in enumerate(tqdm(data_loader, desc=\"Eval\")):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            targets = targets.to(device, non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "            acc1 = accuracy(outputs, targets, topk=(1,))[0]\n",
        "            batch_size = images.size(0)\n",
        "            total_loss = (total_loss * i + loss.item()) / (i + 1)\n",
        "            total_acc = (total_acc * i + acc1) / (i + 1)\n",
        "    return total_loss, total_acc\n",
        "\n",
        "# -------------------------\n",
        "# Main training entrypoint\n",
        "# -------------------------\n",
        "def main():\n",
        "    device = torch.device(cfg.device)\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    train_loader, test_loader = get_dataloaders(cfg.batch_size, cfg.num_workers)\n",
        "\n",
        "    model = ViT(img_size=cfg.image_size,\n",
        "                patch_size=cfg.patch_size,\n",
        "                in_chans=cfg.in_channels,\n",
        "                num_classes=10,\n",
        "                embed_dim=cfg.embed_dim,\n",
        "                depth=cfg.depth,\n",
        "                num_heads=cfg.num_heads,\n",
        "                mlp_ratio=cfg.mlp_ratio,\n",
        "                dropout=cfg.dropout,\n",
        "                attn_dropout=cfg.attn_dropout).to(device)\n",
        "\n",
        "    # optimizer + scheduler (AdamW recommended for ViT)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, betas=cfg.betas, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    niter_per_epoch = len(train_loader)\n",
        "    total_iters = cfg.epochs * niter_per_epoch\n",
        "    warmup_iters = cfg.warmup_epochs * niter_per_epoch\n",
        "    # Create per-step LR schedule\n",
        "    lr_schedule = cosine_scheduler(cfg.lr, 1e-5, cfg.epochs, niter_per_epoch, warmup_epochs=cfg.warmup_epochs, start_warmup_value=1e-6)\n",
        "    # PyTorch requires scheduler.step() every optimizer step; we'll implement a lambda-based scheduler:\n",
        "    lr_iter = iter(lr_schedule)\n",
        "\n",
        "    def _scheduler_step():\n",
        "        try:\n",
        "            lr = next(lr_iter)\n",
        "            for pg in optimizer.param_groups:\n",
        "                pg['lr'] = lr\n",
        "        except StopIteration:\n",
        "            pass\n",
        "\n",
        "    scheduler = _scheduler_step\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.use_amp and device.type == 'cuda')\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(1, cfg.epochs + 1):\n",
        "        train_loss, train_acc = train_one_epoch(model, optimizer, train_loader, device, epoch, scaler=scaler, scheduler=scheduler)\n",
        "        val_loss, val_acc = evaluate(model, test_loader, device)\n",
        "        print(f\"Epoch {epoch}: Train loss {train_loss:.4f}, Train acc {train_acc:.2f}%, Val loss {val_loss:.4f}, Val acc {val_acc:.2f}%\")\n",
        "\n",
        "        # Save best\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            ckpt_path = os.path.join(cfg.save_dir, \"best_vit_cifar10.pth\")\n",
        "            torch.save({\"epoch\": epoch, \"model_state\": model.state_dict(), \"optimizer_state\": optimizer.state_dict(), \"val_acc\": val_acc}, ckpt_path)\n",
        "            print(f\"Saved best model (val acc {best_acc:.2f}%) to {ckpt_path}\")\n",
        "\n",
        "    print(f\"Training finished. Best Val Acc: {best_acc:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "TVShNTgX5sR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32eedd8-bb57-4592-8cb5-86babc0e0e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-203673532.py:336: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=cfg.use_amp and device.type == 'cuda')\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Train Epoch 1:   0%|          | 0/196 [00:00<?, ?it/s]/tmp/ipython-input-203673532.py:252: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "Train Epoch 1:   5%|▍         | 9/196 [04:37<1:32:41, 29.74s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------\n",
        "# 1. Device setup\n",
        "# ------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ------------------------\n",
        "# 2. Data preprocessing\n",
        "# ------------------------\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True,\n",
        "                                             transform=transform_train, download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False,\n",
        "                                            transform=transform_test, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False,\n",
        "                         num_workers=2, pin_memory=True)\n",
        "\n",
        "# ------------------------\n",
        "# 3. Simple ViT (lite version for Colab)\n",
        "# ------------------------\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)                # [B, embed_dim, H/patch, W/patch]\n",
        "        x = x.flatten(2).transpose(1, 2) # [B, num_patches, embed_dim]\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=4, qkv_bias=True):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
        "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        return self.proj(x)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.gelu(self.fc1(x)))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, int(dim * mlp_ratio))\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3,\n",
        "                 num_classes=10, embed_dim=256, depth=6, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads) for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "# ------------------------\n",
        "# 4. Training setup\n",
        "# ------------------------\n",
        "model = VisionTransformer().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type==\"cuda\"))\n",
        "\n",
        "# ------------------------\n",
        "# 5. Training loop\n",
        "# ------------------------\n",
        "def train_one_epoch(model, loader, optimizer, criterion, scaler):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device.type==\"cuda\")):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "        if device.type == \"cuda\":\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        total_correct += outputs.argmax(1).eq(labels).sum().item()\n",
        "    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_correct += outputs.argmax(1).eq(labels).sum().item()\n",
        "    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)\n",
        "\n",
        "# ------------------------\n",
        "# 6. Run training\n",
        "# ------------------------\n",
        "epochs = 10\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n",
        "    val_loss, val_acc = evaluate(model, test_loader, criterion)\n",
        "    print(f\"Epoch {epoch}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xBo5cFW8mDE",
        "outputId": "ced81fc9-264a-4f34-b2ff-2ebfbf1dd0ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [01:15<00:00, 2.26MB/s]\n",
            "Training: 100%|██████████| 391/391 [00:26<00:00, 14.61it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:02<00:00, 27.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Acc=0.3345, Val Acc=0.4334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:24<00:00, 15.73it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:02<00:00, 27.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Acc=0.4650, Val Acc=0.5221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:24<00:00, 15.66it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:03<00:00, 21.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Acc=0.5199, Val Acc=0.5407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:24<00:00, 15.65it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:03<00:00, 25.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Acc=0.5463, Val Acc=0.5750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:24<00:00, 15.79it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:02<00:00, 27.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Acc=0.5687, Val Acc=0.5809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:25<00:00, 15.47it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:05<00:00, 15.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Acc=0.5863, Val Acc=0.6066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:38<00:00, 10.22it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:05<00:00, 15.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Acc=0.6015, Val Acc=0.5995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:33<00:00, 11.74it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:03<00:00, 25.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Acc=0.6134, Val Acc=0.6083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:24<00:00, 15.89it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:03<00:00, 20.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Acc=0.6275, Val Acc=0.6381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:28<00:00, 13.85it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:07<00:00, 11.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Acc=0.6400, Val Acc=0.6463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "trPg4cnuBysC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}